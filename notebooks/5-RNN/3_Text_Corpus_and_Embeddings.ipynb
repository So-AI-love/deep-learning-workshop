{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "3-Text-Corpus-and-Embeddings.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ83tBX0dCG1"
      },
      "source": [
        "# Text Corpus and Embeddings\n",
        "\n",
        "This example trains a RNN to tag words from a corpus - \n",
        "\n",
        "The data used for training is from a Wikipedia download, which is the artificially annotated with parts of speech by the NLTK PoS tagger written by Matthew Honnibal.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HniZT7aAdCG6"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "SENTENCE_LENGTH_MAX = 32\n",
        "EMBEDDING_DIM=50"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boNnSYHfdCG8"
      },
      "source": [
        "## Basic Text and Parsing Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B39CjeJfdCG9",
        "outputId": "7ef89863-d4b9-4749-a26c-55df8db43b50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXLr_ryhdCG-",
        "outputId": "6cdd550d-7724-4ee4-8e00-08369147fbec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sentence_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "sentence_splitter.tokenize(\"This is Mr. Smith's tokenized test. The U.S.A gives us sent two. Is this sent three?\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"This is Mr. Smith's tokenized test.\",\n",
              " 'The U.S.A gives us sent two.',\n",
              " 'Is this sent three?']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mTkQL70dCG_",
        "outputId": "055792ca-d4bd-4ebc-ec0e-c18533800ce4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(\"This is Mr. Smith's tokenized test.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'Mr.', 'Smith', \"'s\", 'tokenized', 'test', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOLxoZ4ndCG_"
      },
      "source": [
        "### Download a Wikipedia Corpus\n",
        "\n",
        "From the corpus download page : http://wortschatz.uni-leipzig.de/en/download/\n",
        "\n",
        "Here's the paper that explains how the corpus was constructed : \n",
        "\n",
        "*  D. Goldhahn, T. Eckart & U. Quasthoff: Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages.\n",
        "    *  In: Proceedings of the 8th International Language Ressources and Evaluation (LREC'12), 2012\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDnimShidCHB"
      },
      "source": [
        "corpus_dir = './data/RNN/'\n",
        "corpus_text_file = os.path.join(corpus_dir, 'en.wikipedia.2010.100K.txt')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb_S99iodCHC",
        "outputId": "2c8aa4f7-22fb-4deb-dc85-e98e71635a6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if not os.path.isfile( corpus_text_file ):\n",
        "    if not os.path.exists(corpus_dir):\n",
        "        os.makedirs(corpus_dir)\n",
        "\n",
        "    corpus_text_tar = 'eng_wikipedia_2010_100K.tar.gz'    \n",
        "    download_url = 'http://pcai056.informatik.uni-leipzig.de/downloads/corpora/'+corpus_text_tar\n",
        "\n",
        "    data_cache = './data/cache'\n",
        "    if not os.path.exists(data_cache):\n",
        "        os.makedirs(data_cache)\n",
        "    \n",
        "    # Fall-back url if too slow\n",
        "    #download_url= 'http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/data/RNN/'+corpus_text_tar\n",
        "\n",
        "    import shutil, requests\n",
        "\n",
        "    # Get the download path from the web-service\n",
        "    #urllib.request.urlretrieve('http://wortschatz.uni-leipzig.de/download/service', corpus_text_tar)\n",
        "    # download_url = ...\n",
        "    \n",
        "    tarfilepath = os.path.join(data_cache, corpus_text_tar)\n",
        "    if not os.path.isfile( tarfilepath ):\n",
        "        response = requests.get(download_url, stream=True)\n",
        "        with open(tarfilepath, 'wb') as out_file:\n",
        "            shutil.copyfileobj(response.raw, out_file)\n",
        "    if os.path.isfile(tarfilepath):\n",
        "        import tarfile\n",
        "        #tarfile.open(tarfilepath, 'r:gz').extractall(corpus_dir)\n",
        "        tarfile.open(tarfilepath, 'r:gz').extract('eng_wikipedia_2010_100K-sentences.txt', corpus_dir)\n",
        "    shutil.move(os.path.join(corpus_dir, 'eng_wikipedia_2010_100K-sentences.txt'), corpus_text_file)\n",
        "    \n",
        "    # Get rid of tarfile source (the required text file itself will remain)\n",
        "    #os.unlink(tarfilepath)\n",
        "\n",
        "print(\"Corpus available locally\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus available locally\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKq9TuFudCHD"
      },
      "source": [
        "## This is a work-in-progress, since we should really discover 'download_url' from the 'service'\n",
        "#r=requests.post('http://wortschatz.uni-leipzig.de/download/service', data='file=%s&func=\"link\"' % (corpus_text_tar,))\n",
        "#r=requests.post('http://wortschatz.uni-leipzig.de/download/service', data=dict(file=corpus_text_tar, func=\"link\") )\n",
        "#r.text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-KHH5sudCHE"
      },
      "source": [
        "def corpus_sentence_tokens(corpus_text_file=corpus_text_file):\n",
        "    while True:\n",
        "        with open(corpus_text_file, encoding='utf-8') as f:\n",
        "            for line in f.readlines():\n",
        "                n,l = line.split('\\t')   # Strip of the initial numbers\n",
        "                for s in sentence_splitter.tokenize(l):  # Split the lines into sentences (~1 each)\n",
        "                    tree_banked = tokenizer.tokenize(s)\n",
        "                    if len(tree_banked) < SENTENCE_LENGTH_MAX:\n",
        "                        yield tree_banked\n",
        "        print(\"Corpus : Looping\")\n",
        "corpus_sentence_tokens_gen = corpus_sentence_tokens()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mJ8TYnodCHE",
        "outputId": "d57dbad8-b80e-40c9-9e6b-24e4337b6382",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "' | '.join(next(corpus_sentence_tokens_gen))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Showing | that | even | in | the | modern | warfare | of | the | 1930s | and | 1940s | , | the | dilapidated | fortifications | still | had | defensive | usefulness | .'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHiQOLpFdCHF"
      },
      "source": [
        "## GloVe Word Embeddings\n",
        "Using the python package :  https://github.com/maciejkula/glove-python , and code samples from : http://developers.lyst.com/2014/11/11/word-embeddings-for-fashion/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwRw9N4IdCHG",
        "outputId": "bb4ee39e-4a86-4785-a3d8-947b3c7b23b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install glove-python-binary"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: glove-python-binary in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym6XuMpBkmNw",
        "outputId": "ee70f3eb-f642-440c-adfa-430aab0ff4e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -U mittens\n",
        "!pip uninstall tensorflow -y\n",
        "!pip install tensorflow==1.13.2"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mittens in /usr/local/lib/python3.7/dist-packages (0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mittens) (1.19.5)\n",
            "Found existing installation: tensorflow 1.13.2\n",
            "Uninstalling tensorflow-1.13.2:\n",
            "  Successfully uninstalled tensorflow-1.13.2\n",
            "Collecting tensorflow==1.13.2\n",
            "  Using cached tensorflow-1.13.2-cp37-cp37m-manylinux1_x86_64.whl (92.7 MB)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.8.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.13.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.19.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.39.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.37.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.12.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.13.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (3.17.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (4.6.4)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.2) (4.0.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.2) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.7.4.3)\n",
            "Installing collected packages: tensorflow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.13.2 which is incompatible.\u001b[0m\n",
            "Successfully installed tensorflow-1.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUnnLyLBdCHG"
      },
      "source": [
        "### Create the Co-occurrence Matrix\n",
        "For speed, this looks at the first 100,000 tokens in the corpus - and should create the co-occurences in 30 seconds or so."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArCKVkSDdCHG",
        "outputId": "3217cc2a-a698-419b-aefb-9ff957421b99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import glove\n",
        "glove_corpus = glove.Corpus()\n",
        "\n",
        "corpus_sentences = [ \n",
        "        [ w.lower() for w in next(corpus_sentence_tokens_gen)] # All lower-case\n",
        "        for _ in range(0,100*1000) \n",
        "    ]\n",
        "\n",
        "# Fit the co-occurrence matrix using a sliding window of 10 words.\n",
        "t0 = time.time()\n",
        "glove_corpus.fit(corpus_sentences, window=10)\n",
        "\n",
        "print(\"Dictionary length=%d\" % (len(glove_corpus.dictionary),))\n",
        "print(\"Co-occurrence calculated in %5.1fsec\" % (time.time()-t0, ))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus : Looping\n",
            "Dictionary length=98815\n",
            "Co-occurrence calculated in   7.7sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka_fK-BXdCHH",
        "outputId": "262fcda4-6707-43b1-eec5-7c676b46b957",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Return the index of the word in the dictionary\n",
        "glove_corpus.dictionary['city']"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "544"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0vzXwnZdCHI"
      },
      "source": [
        "###  Create the Word Embedding\n",
        "\n",
        "This will make use of up to 4 threads - and each epoch takes 20-30 seconds on a single core."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILUNhb7pdCHI",
        "outputId": "eabfd583-e989-47a0-f0b9-37290000a2cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_embedding = glove.Glove(no_components=EMBEDDING_DIM, learning_rate=0.05)\n",
        "\n",
        "t0 = time.time()\n",
        "glove_epochs, glove_threads = 20, 4 \n",
        "\n",
        "word_embedding.fit(glove_corpus.matrix, epochs=glove_epochs, no_threads=glove_threads, verbose=True)\n",
        "\n",
        "print(\"%d-d word-embedding created in %5.1fsec = %5.1fsec per epoch\" % (\n",
        "        EMBEDDING_DIM, (time.time()-t0), (time.time()-t0)/glove_epochs*glove_threads, ))\n",
        "\n",
        "# Add the word -> id dictionary to the model to allow similarity queries.\n",
        "word_embedding.add_dictionary(glove_corpus.dictionary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing 20 training epochs with 4 threads\n",
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GldcBTo0dCHJ"
      },
      "source": [
        "#word_embedding.save(\"./data/RNN/glove.embedding.50.pkl\")\n",
        "#word_embedding.load(\"./data/RNN/glove.embedding.50.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXsFBjxKdCHJ"
      },
      "source": [
        "###  Test Word Embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f90KAkGRdCHJ"
      },
      "source": [
        "# word-similarity test\n",
        "word_embedding.most_similar('country')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxCkqesNdCHJ"
      },
      "source": [
        "# word-analogy test\n",
        "def get_embedding_vec(word):\n",
        "    idx = word_embedding.dictionary.get(word.lower(), -1)\n",
        "    if idx<0:\n",
        "        #print(\"Missing word : '%s'\" % (word,))\n",
        "        return np.zeros(  (EMBEDDING_DIM, ), dtype='float32')  # UNK\n",
        "    return word_embedding.word_vectors[idx]\n",
        "\n",
        "def get_closest_word(vec, number=5):\n",
        "    dst = (np.dot(word_embedding.word_vectors, vec)\n",
        "                   / np.linalg.norm(word_embedding.word_vectors, axis=1)\n",
        "                   / np.linalg.norm(vec))\n",
        "    word_ids = np.argsort(-dst)\n",
        "    return [(word_embedding.inverse_dictionary[x], dst[x]) for x in word_ids[:number]\n",
        "            if x in word_embedding.inverse_dictionary]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gcki-uyPdCHK"
      },
      "source": [
        "analogy_vec = get_embedding_vec('woman') + get_embedding_vec('king') - get_embedding_vec('man')\n",
        "get_closest_word(analogy_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LyUAiGZdCHK"
      },
      "source": [
        "def test_analogy(s='one two three four'):\n",
        "    (a,b,c,d) = s.split(' ')\n",
        "    analogy_vec = get_embedding_vec(b) - get_embedding_vec(a) + get_embedding_vec(c)\n",
        "    words = [ w for (w,p) in get_closest_word(analogy_vec) if w not in (a,b,c)]\n",
        "    print(\"'%s' is to '%s' as '%s' is to {%s}\" % (a,b,c,', '.join(words)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMBHmWvAdCHL"
      },
      "source": [
        "test_analogy('man woman king queen')\n",
        "test_analogy('paris france rome italy')\n",
        "test_analogy('kitten cat puppy dog')\n",
        "test_analogy('understand understood run ran')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbtEuN-kdCHL"
      },
      "source": [
        "### Problem : Embedding is *Poor*\n",
        "### Solution : Load a pre-trained word embedding\n",
        "\n",
        "Since the embedding we learnt above is poor, let's load a pre-trained word embedding, from a much larger corpus, trained for a much longer period.  Source of this word embedding (created from a 6 billion tokens corpus, with results as 50d vectors): http://nlp.stanford.edu/projects/glove/ \n",
        "\n",
        "NB: If you don't have the required data, and the RedCatLabs server doesn't give you the download, the loader below downloads a 823Mb file via a fairly slow connection to a server at Stanford (this can take HOURS)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB85B--cdCHL"
      },
      "source": [
        "import os, requests, shutil\n",
        "\n",
        "glove_dir = './data/RNN/'\n",
        "glove_100k_50d = 'glove.first-100k.6B.50d.txt'\n",
        "glove_100k_50d_path = os.path.join(glove_dir, glove_100k_50d)\n",
        "\n",
        "# These are temporary files if we need to download it from the original source (slow)\n",
        "data_cache = './data/cache'\n",
        "glove_full_tar = 'glove.6B.zip'\n",
        "glove_full_50d = 'glove.6B.50d.txt'\n",
        "\n",
        "#force_download_from_original=False\n",
        "download_url= 'http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/data/RNN/'+glove_100k_50d\n",
        "original_url = 'http://nlp.stanford.edu/data/'+glove_full_tar\n",
        "\n",
        "if not os.path.isfile( glove_100k_50d_path ):\n",
        "    if not os.path.exists(glove_dir):\n",
        "        os.makedirs(glove_dir)\n",
        "    \n",
        "    # First, try to download a pre-prepared file directly...\n",
        "    response = requests.get(download_url, stream=True)\n",
        "    if response.status_code == requests.codes.ok:\n",
        "        print(\"Downloading 42Mb pre-prepared GloVE file from RedCatLabs\")\n",
        "        with open(glove_100k_50d_path, 'wb') as out_file:\n",
        "            shutil.copyfileobj(response.raw, out_file)\n",
        "    else:\n",
        "        # But, for some reason, RedCatLabs didn't give us the file directly\n",
        "        if not os.path.exists(data_cache):\n",
        "            os.makedirs(data_cache)\n",
        "        \n",
        "        if not os.path.isfile( os.path.join(data_cache, glove_full_50d) ):\n",
        "            zipfilepath = os.path.join(data_cache, glove_full_tar)\n",
        "            if not os.path.isfile( zipfilepath ):\n",
        "                print(\"Downloading 860Mb GloVE file from Stanford\")\n",
        "                response = requests.get(download_url, stream=True)\n",
        "                with open(zipfilepath, 'wb') as out_file:\n",
        "                    shutil.copyfileobj(response.raw, out_file)\n",
        "            if os.path.isfile(zipfilepath):\n",
        "                print(\"Unpacking 50d GloVE file from zip\")\n",
        "                import zipfile\n",
        "                zipfile.ZipFile(zipfilepath, 'r').extract(glove_full_50d, data_cache)\n",
        "\n",
        "        with open(os.path.join(data_cache, glove_full_50d), 'rt') as in_file:\n",
        "            with open(glove_100k_50d_path, 'wt') as out_file:\n",
        "                print(\"Reducing 50d GloVE file to first 100k words\")\n",
        "                for i, l in enumerate(in_file.readlines()):\n",
        "                    if i>=100000: break\n",
        "                    out_file.write(l)\n",
        "    \n",
        "        # Get rid of tarfile source (the required text file itself will remain)\n",
        "        #os.unlink(zipfilepath)\n",
        "        #os.unlink(os.path.join(data_cache, glove_full_50d))\n",
        "\n",
        "print(\"GloVE available locally\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFoQ7mOdCHM"
      },
      "source": [
        "# Due to size constraints, only use the first 100k vectors (i.e. 100k most frequently used words)\n",
        "word_embedding = glove.Glove.load_stanford( glove_100k_50d_path )\n",
        "word_embedding.word_vectors.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-U8t2FKdCHN"
      },
      "source": [
        "Having loaded that, play around with the similarity and analogy tests again..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kppqB_dYdCHN"
      },
      "source": [
        "word_embedding.most_similar('king')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJo9tQkkdCHN"
      },
      "source": [
        "test_analogy('man woman king queen')\n",
        "test_analogy('paris france rome italy')\n",
        "test_analogy('kitten cat puppy dog')\n",
        "test_analogy('understand understood run ran')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAkdLWArdCHO"
      },
      "source": [
        "### Visualize Embedding in TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4HFjsUJm1nT"
      },
      "source": [
        "# !  pip uninstall tensorflow_estimator\n",
        "# !  pip install tensorflow_estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNETFo9tdCHO"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib.tensorboard.plugins import projector\n",
        "\n",
        "#N = 10000 # Number of items (vocab size).\n",
        "#D = 200 # Dimensionality of the embedding.\n",
        "#embedding_var = tf.Variable(tf.random_normal([N,D]), name='word_embedding')\n",
        "\n",
        "embedding_var = tf.Variable(word_embedding.word_vectors, dtype='float32', \n",
        "                            name='word_embedding')\n",
        "\n",
        "# Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto\n",
        "projector_config = projector.ProjectorConfig()\n",
        "\n",
        "# You can add multiple embeddings. Here we add only one.\n",
        "embedding = projector_config.embeddings.add()\n",
        "embedding.tensor_name = embedding_var.name\n",
        "\n",
        "# Link this tensor to its metadata file (e.g. labels).\n",
        "LOG_DIR='../../tensorflow.logdir/'\n",
        "os.makedirs(LOG_DIR, exist_ok=True)    \n",
        "\n",
        "metadata_file = 'glove_full_50d.words.tsv'\n",
        "vocab_list = [ word_embedding.inverse_dictionary[i] \n",
        "               for i in range(len( word_embedding.inverse_dictionary )) ]\n",
        "with open(os.path.join(LOG_DIR, metadata_file), 'wt') as metadata:\n",
        "    metadata.writelines(\"%s\\n\" % w for w in vocab_list)\n",
        "    \n",
        "embedding.metadata_path = os.path.join(os.getcwd(), LOG_DIR, metadata_file)\n",
        "\n",
        "# Use the same LOG_DIR where you stored your checkpoint.\n",
        "summary_writer = tf.summary.FileWriter(LOG_DIR)\n",
        "\n",
        "# The next line writes a projector_config.pbtxt in the LOG_DIR. TensorBoard will\n",
        "# read this file during startup.\n",
        "projector.visualize_embeddings(summary_writer, projector_config)\n",
        "\n",
        "saver = tf.train.Saver([embedding_var])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Initialize the model\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    saver.save(sess, os.path.join(LOG_DIR, metadata_file+'.ckpt'))\n",
        "#print(\"Look at the embedding in TensorBoard : http://localhost:8081/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nIq-IFRdCHP"
      },
      "source": [
        "### Run TensorBoard via Colab "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tbZDHMQdCHP"
      },
      "source": [
        "# Start the tensorboard server on this (colab) machine\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 8081 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXqaYkd8dCHP"
      },
      "source": [
        "# Install 'localtunnel' (a node.js proxy) -- work a little harder to avoid global install\n",
        "! npm install localtunnel\n",
        "\n",
        "! ls -l node_modules/localtunnel/bin/client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfYRH89xdCHQ"
      },
      "source": [
        "# Tunnel port 8081 (TensorBoard assumed running)\n",
        "get_ipython().system_raw('node_modules/localtunnel/bin/client --port 8081 >> tunnel_url.txt 2>&1 &')\n",
        "\n",
        "# Check that it's running\n",
        "! ps fax | grep node | grep 8081"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KNpgvl-dCHQ"
      },
      "source": [
        "# Get url - this should be available on the web \n",
        "#   (tunnels into colab via localtunnel to its tensorboard)\n",
        "! cat tunnel_url.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gemr64q2dCHQ"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "1.  Plot some of the embeddings on a graph (potentially apply PCA first)\n",
        "\n",
        "    +  Nice example \"medical\"\n",
        "\n",
        "2.  ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs0CazwzdCHR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}